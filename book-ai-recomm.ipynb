{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:10:23.368726Z",
     "iopub.status.busy": "2025-12-28T16:10:23.368367Z",
     "iopub.status.idle": "2025-12-28T16:10:27.214165Z",
     "shell.execute_reply": "2025-12-28T16:10:27.213178Z",
     "shell.execute_reply.started": "2025-12-28T16:10:23.368695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch sentence-transformers gradio requests beautifulsoup4 langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:10:27.216744Z",
     "iopub.status.busy": "2025-12-28T16:10:27.216222Z",
     "iopub.status.idle": "2025-12-28T16:10:30.644260Z",
     "shell.execute_reply": "2025-12-28T16:10:30.643510Z",
     "shell.execute_reply.started": "2025-12-28T16:10:27.216708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:10:30.645991Z",
     "iopub.status.busy": "2025-12-28T16:10:30.645686Z",
     "iopub.status.idle": "2025-12-28T16:10:33.867194Z",
     "shell.execute_reply": "2025-12-28T16:10:33.866288Z",
     "shell.execute_reply.started": "2025-12-28T16:10:30.645947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:10:33.869690Z",
     "iopub.status.busy": "2025-12-28T16:10:33.869390Z",
     "iopub.status.idle": "2025-12-28T16:10:33.875345Z",
     "shell.execute_reply": "2025-12-28T16:10:33.874482Z",
     "shell.execute_reply.started": "2025-12-28T16:10:33.869662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:25.889883Z",
     "iopub.status.busy": "2025-12-28T16:12:25.889359Z",
     "iopub.status.idle": "2025-12-28T16:12:25.896191Z",
     "shell.execute_reply": "2025-12-28T16:12:25.895635Z",
     "shell.execute_reply.started": "2025-12-28T16:12:25.889853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class OpenSourceLLM:\n",
    "    def __init__(self):\n",
    "        print(\"Real LLM loaded ‚Äî ready to recommend like a human\")\n",
    "        self.api_key = \"your_key_here\n",
    "        self.api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        self.model = \"llama-3.3-70b-versatile\"  \n",
    "\n",
    "    def generate(self, prompt, max_tokens=200):\n",
    "        if not self.api_key:\n",
    "            return \"Sweet romance with amazing chemistry and all the feels!\"\n",
    "    \n",
    "        import requests\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key.strip()}\",  \n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            print(f\"Groq status: {response.status_code}\")  \n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                print(f\"Groq error details: {response.text}\")  # Shows exact error\n",
    "                return \"Sweet romance with amazing chemistry and all the feels!\"\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return \"Sweet romance with amazing chemistry and all the feels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:27.796169Z",
     "iopub.status.busy": "2025-12-28T16:12:27.795873Z",
     "iopub.status.idle": "2025-12-28T16:12:27.800534Z",
     "shell.execute_reply": "2025-12-28T16:12:27.799737Z",
     "shell.execute_reply.started": "2025-12-28T16:12:27.796141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real LLM loaded ‚Äî ready to recommend like a human\n"
     ]
    }
   ],
   "source": [
    "llm = OpenSourceLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:31.091655Z",
     "iopub.status.busy": "2025-12-28T16:12:31.091249Z",
     "iopub.status.idle": "2025-12-28T16:12:31.106046Z",
     "shell.execute_reply": "2025-12-28T16:12:31.105414Z",
     "shell.execute_reply.started": "2025-12-28T16:12:31.091622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BookSearchEngine:\n",
    "    \"\"\"Search books from multiple sources on the web\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.google_books_url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "        self.openlibrary_url = \"https://openlibrary.org/search.json\"\n",
    "    \n",
    "    def search_google_books(self, query, max_results=10):\n",
    "        \"\"\"Search Google Books API\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'maxResults': max_results,\n",
    "                'printType': 'books',\n",
    "                'orderBy': 'relevance'\n",
    "            }\n",
    "            response = requests.get(self.google_books_url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            books = []\n",
    "            for item in data.get('items', []):\n",
    "                vol_info = item.get('volumeInfo', {})\n",
    "                books.append({\n",
    "                    'title': vol_info.get('title', 'Unknown'),\n",
    "                    'authors': ', '.join(vol_info.get('authors', ['Unknown'])),\n",
    "                    'description': vol_info.get('description', 'No description available'),\n",
    "                    'categories': ', '.join(vol_info.get('categories', ['General'])),\n",
    "                    'published': vol_info.get('publishedDate', 'N/A'),\n",
    "                    'rating': vol_info.get('averageRating', 'N/A'),\n",
    "                    'thumbnail': vol_info.get('imageLinks', {}).get('thumbnail', ''),\n",
    "                    'source': 'Google Books'\n",
    "                })\n",
    "            return books\n",
    "        except Exception as e:\n",
    "            print(f\"Google Books error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_openlibrary(self, query, max_results=10):\n",
    "        \"\"\"Search Open Library API\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'limit': max_results,\n",
    "                'fields': 'title,author_name,first_publish_year,subject,ratings_average'\n",
    "            }\n",
    "            response = requests.get(self.openlibrary_url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            books = []\n",
    "            for doc in data.get('docs', []):\n",
    "                books.append({\n",
    "                    'title': doc.get('title', 'Unknown'),\n",
    "                    'authors': ', '.join(doc.get('author_name', ['Unknown'])),\n",
    "                    'description': ', '.join(doc.get('subject', ['No description'])[:3]),\n",
    "                    'categories': ', '.join(doc.get('subject', ['General'])[:2]),\n",
    "                    'published': doc.get('first_publish_year', 'N/A'),\n",
    "                    'rating': doc.get('ratings_average', 'N/A'),\n",
    "                    'thumbnail': '',\n",
    "                    'source': 'Open Library'\n",
    "                })\n",
    "            return books\n",
    "        except Exception as e:\n",
    "            print(f\"Open Library error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_books(self, query, max_results=15):\n",
    "        \"\"\"Search books from multiple sources\"\"\"\n",
    "        print(f\"Searching for: {query}\")\n",
    "        \n",
    "        # Search both APIs\n",
    "        google_results = self.search_google_books(query, max_results=max_results//2)\n",
    "        openlibrary_results = self.search_openlibrary(query, max_results=max_results//2)\n",
    "        \n",
    "        # Combine results\n",
    "        all_books = google_results + openlibrary_results\n",
    "        \n",
    "        # Remove duplicates based on title similarity\n",
    "        unique_books = []\n",
    "        seen_titles = set()\n",
    "        for book in all_books:\n",
    "            title_lower = book['title'].lower()\n",
    "            if title_lower not in seen_titles:\n",
    "                seen_titles.add(title_lower)\n",
    "                unique_books.append(book)\n",
    "        \n",
    "        print(f\"Found {len(unique_books)} unique books\")\n",
    "        return unique_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:31.235266Z",
     "iopub.status.busy": "2025-12-28T16:12:31.235025Z",
     "iopub.status.idle": "2025-12-28T16:12:31.238724Z",
     "shell.execute_reply": "2025-12-28T16:12:31.237986Z",
     "shell.execute_reply.started": "2025-12-28T16:12:31.235242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "search_engine = BookSearchEngine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:34.063165Z",
     "iopub.status.busy": "2025-12-28T16:12:34.062661Z",
     "iopub.status.idle": "2025-12-28T16:12:35.212411Z",
     "shell.execute_reply": "2025-12-28T16:12:35.211808Z",
     "shell.execute_reply.started": "2025-12-28T16:12:34.063134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: science fiction space\n",
      "Found 4 unique books\n",
      "\n",
      "Test search found 4 books:\n",
      "- Science Fiction Literature through History by Gary Westfahl\n",
      "- Science Fiction and Space Futures by Eugene Morlock Emme\n",
      "- 2001 by Arthur C. Clarke\n"
     ]
    }
   ],
   "source": [
    "test_results = search_engine.search_books(\"science fiction space\", max_results=5)\n",
    "print(f\"\\nTest search found {len(test_results)} books:\")\n",
    "for book in test_results[:3]:\n",
    "    print(f\"- {book['title']} by {book['authors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:36.214463Z",
     "iopub.status.busy": "2025-12-28T16:12:36.214132Z",
     "iopub.status.idle": "2025-12-28T16:12:37.653516Z",
     "shell.execute_reply": "2025-12-28T16:12:37.652752Z",
     "shell.execute_reply.started": "2025-12-28T16:12:36.214432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Embedding model ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:38.041095Z",
     "iopub.status.busy": "2025-12-28T16:12:38.040518Z",
     "iopub.status.idle": "2025-12-28T16:12:38.046753Z",
     "shell.execute_reply": "2025-12-28T16:12:38.045937Z",
     "shell.execute_reply.started": "2025-12-28T16:12:38.041064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_book_embeddings(books):\n",
    "    \"\"\"Create embeddings for book search results\"\"\"\n",
    "    if not books:\n",
    "        return np.array([])\n",
    "    \n",
    "    texts = []\n",
    "    for book in books:\n",
    "        text = f\"{book['title']}. {book['description']} {book['categories']}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def semantic_rerank(books, query, top_k=5):\n",
    "    \"\"\"Rerank books using semantic similarity\"\"\"\n",
    "    if not books:\n",
    "        return []\n",
    "    \n",
    "    book_embeddings = create_book_embeddings(books)\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    ranked_books = [books[i] for i in top_indices]\n",
    "    return ranked_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:12:40.074943Z",
     "iopub.status.busy": "2025-12-28T16:12:40.074150Z",
     "iopub.status.idle": "2025-12-28T16:12:40.095232Z",
     "shell.execute_reply": "2025-12-28T16:12:40.094443Z",
     "shell.execute_reply.started": "2025-12-28T16:12:40.074908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-Powered Book Recommender Ready! üöÄ\n"
     ]
    }
   ],
   "source": [
    "class AIBookRecommender:\n",
    "    def __init__(self, llm, search_engine, embedding_model):\n",
    "        self.llm = llm\n",
    "        self.search_engine = search_engine\n",
    "        self.embedding_model = embedding_model\n",
    "        self.reading_list = []\n",
    "        self.last_search_results = []\n",
    "        self.last_embeddings = None\n",
    "\n",
    "    def create_book_embeddings(self, books):\n",
    "        \"\"\"Create embeddings for book search results\"\"\"\n",
    "        if not books:\n",
    "            return np.array([])\n",
    "        \n",
    "        texts = []\n",
    "        for book in books:\n",
    "            text = f\"{book['title']}. {book['description']} {book['categories']}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def semantic_rerank(self, books, query, top_k=20):\n",
    "        \"\"\"Rerank books using semantic similarity\"\"\"\n",
    "        if not books:\n",
    "            return []\n",
    "        \n",
    "        book_embeddings = self.create_book_embeddings(books)\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        ranked_books = [books[i] for i in top_indices]\n",
    "        return ranked_books\n",
    "\n",
    "    def extract_intent(self, user_message):\n",
    "        lower = user_message.lower()\n",
    "\n",
    "        if not any(phrase in lower for phrase in [\"like\", \"similar\", \"recommend\", \"loved\", \"read\", \"want\", \"something\"]):\n",
    "            return user_message + \" books\", None\n",
    "\n",
    "        intent_prompt = f\"\"\"\n",
    "User message: \"{user_message}\"\n",
    "\n",
    "Extract:\n",
    "1. The exact book title they want something similar to (if mentioned). Return \"None\" if not clear.\n",
    "2. List of key tropes, genres, or vibes they mentioned or implied (e.g. fake dating, enemies to lovers, dark academia, cozy fantasy, slow burn, found family).\n",
    "\n",
    "Format exactly as:\n",
    "BOOK: <title or None>\n",
    "TROPES: trope1, trope2, trope3\n",
    "\n",
    "Only respond with those two lines.\n",
    "\"\"\"\n",
    "\n",
    "        response = self.llm.generate(intent_prompt, max_tokens=100).strip()\n",
    "\n",
    "        # Parse LLM response\n",
    "        book_title = None\n",
    "        tropes = []\n",
    "        for line in response.split('\\n'):\n",
    "            if line.startswith(\"BOOK:\"):\n",
    "                title = line[5:].strip()\n",
    "                if title.lower() != \"none\":\n",
    "                    book_title = title\n",
    "            elif line.startswith(\"TROPES:\"):\n",
    "                trope_str = line[7:].strip()\n",
    "                if trope_str:\n",
    "                    tropes = [t.strip() for t in trope_str.split(',') if t.strip()]\n",
    "\n",
    "        if not book_title:\n",
    "            import re\n",
    "            match = re.search(r\"(?:like|similar to|loved|read|something like)\\s+([a-zA-Z0-9\\s,'\\\"\\-\\.&!]+?)(?:\\s+by|\\?|$|,)\", lower)\n",
    "            if match:\n",
    "                book_title = match.group(1).strip().title()\n",
    "\n",
    "        # Fallback: keyword trope detection\n",
    "        if not tropes:\n",
    "            common_checks = {\n",
    "                'fake dating': ['fake dat', 'fake relat', 'pretend', 'fake boyfriend', 'fake girlfriend'],\n",
    "                'friends to lovers': ['friend', 'best friend', 'childhood friend', 'friends to lovers'],\n",
    "                'enemies to lovers': ['enem', 'hate to love', 'rival'],\n",
    "                'grumpy sunshine': ['grumpy', 'sunshine'],\n",
    "                'slow burn': ['slow burn', 'slowburn'],\n",
    "                'romcom': ['romcom', 'romantic comedy', 'funny romance'],\n",
    "                'young adult': ['ya', 'young adult', 'teen'],\n",
    "                'new adult': ['new adult', 'college'],\n",
    "                'dark academia': ['dark academia', 'academic'],\n",
    "                'cozy fantasy': ['cozy', 'wholesome fantasy'],\n",
    "                'closed door': ['clean', 'closed door', 'no spice', 'sweet'],\n",
    "            }\n",
    "            for trope, keywords in common_checks.items():\n",
    "                if any(k in lower for k in keywords):\n",
    "                    tropes.append(trope)\n",
    "\n",
    "        # Build powerful search query\n",
    "        parts = []\n",
    "        if book_title:\n",
    "            parts.append(f'\"{book_title}\"')\n",
    "            parts.append(\"similar books\")\n",
    "            parts.append(\"read alikes\")\n",
    "            parts.append(\"goodreads recommendations\")\n",
    "            parts.append(\"booktok recommendations\")\n",
    "        else:\n",
    "            parts.append(\"books\")\n",
    "\n",
    "        if tropes:\n",
    "            parts.extend(tropes)\n",
    "\n",
    "        if \"book\" not in \" \".join(parts).lower():\n",
    "            parts.append(\"book\")\n",
    "\n",
    "        query = \" \".join(parts)\n",
    "        return query, book_title\n",
    "\n",
    "    def chat(self, user_message, history=[]):\n",
    "        user_message_lower = user_message.lower().strip()\n",
    "    \n",
    "        # === COMMAND: Add book to reading list ===\n",
    "        if user_message_lower.startswith(\"add \"):\n",
    "            title_to_add = user_message[4:].strip()\n",
    "            added = False\n",
    "            for book in self.last_search_results:  \n",
    "                if title_to_add.lower() in book['title'].lower():\n",
    "                    if book not in self.reading_list:\n",
    "                        self.reading_list.append(book)\n",
    "                    return f\"‚úÖ Added **{book['title']}** by {book['authors']} to your reading list!\"\n",
    "                    added = True\n",
    "            if not added:\n",
    "                return \"‚ùå Couldn't find that book in recent recs. Try copying the exact title.\"\n",
    "    \n",
    "        if user_message_lower == \"reading list\":\n",
    "            if not self.reading_list:\n",
    "                return \"üìö Your reading list is empty. Start adding some bangers.\"\n",
    "            response = \"üìö **Your Reading List:**\\n\\n\"\n",
    "            for i, book in enumerate(self.reading_list, 1):\n",
    "                response += f\"**{i}. {book['title']}** by {book['authors']}\\n\"\n",
    "                response += f\"üìñ {book['description'][:160]}...\\n\\n\"\n",
    "            return response\n",
    "    \n",
    "        prompt = f\"\"\"\n",
    "    You are a brutally honest, no-bullshit book recommender who actually listens.\n",
    "    User says: \"{user_message}\"\n",
    "    \n",
    "    Give exactly 5 book recommendations that genuinely match what they're asking for ‚Äî nothing safe, nothing generic.\n",
    "    For each book:\n",
    "    - Title\n",
    "    - Author\n",
    "    - One raw, real sentence explaining why it fits (vibes, themes, energy ‚Äî be direct)\n",
    "    \n",
    "    No disclaimers. No \"it depends on your taste\". No moral lectures. Just cold, hard truth recs.\n",
    "    \"\"\"\n",
    "    \n",
    "        raw_response = self.llm.generate(prompt, max_tokens=500)\n",
    "    \n",
    "        if not raw_response or \"error\" in raw_response.lower():\n",
    "            raw_response = \"LLM's being a little bitch right now, but trust ‚Äî I'd have given you fire recs.\"\n",
    "    \n",
    "        final_response = f\"üìö **Straight-up recs for you:**\\n\\n{raw_response}\\n\\nüí¨ Commands: `add [title]` to save ¬∑ `reading list` to view saved\"\n",
    "    \n",
    "        return final_response\n",
    "recommender = AIBookRecommender(llm, search_engine, embedding_model)\n",
    "print(\"AI-Powered Book Recommender Ready! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-28T16:22:04.808956Z",
     "iopub.status.busy": "2025-12-28T16:22:04.808616Z",
     "iopub.status.idle": "2025-12-28T16:22:04.980829Z",
     "shell.execute_reply": "2025-12-28T16:22:04.980251Z",
     "shell.execute_reply.started": "2025-12-28T16:22:04.808924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"#  AI Book Recommender \\n###\")\n",
    "    chatbot = gr.Chatbot(height=500, type=\"messages\")  \n",
    "    msg = gr.Textbox(placeholder=\"Tell me what you like...\", show_label=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ### Try:\n",
    "    - \"I want a book like the love hypothesis\"\n",
    "    - \"fake dating grumpy sunshine\"\n",
    "    - \"dark academia enemies to lovers\"\n",
    "    - \"add [book title]\" to save\n",
    "    - \"reading list\" to view saved books\n",
    "    \"\"\")\n",
    "\n",
    "    def respond(message, history):\n",
    "        response = recommender.chat(message, history)\n",
    "        history.append({\"role\": \"user\", \"content\": message})\n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return \"\", history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: [], None, chatbot) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T16:22:10.792405Z",
     "iopub.status.busy": "2025-12-28T16:22:10.792150Z",
     "iopub.status.idle": "2025-12-28T16:30:19.857507Z",
     "shell.execute_reply": "2025-12-28T16:30:19.856747Z",
     "shell.execute_reply.started": "2025-12-28T16:22:10.792382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://61eab0d761212545b2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://61eab0d761212545b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq status: 200\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://61eab0d761212545b2.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.queue()\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
