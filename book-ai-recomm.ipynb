{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Book recommendation system","metadata":{}},{"cell_type":"markdown","source":"## Imports & set up","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers torch sentence-transformers gradio requests beautifulsoup4 langchain langchain-community\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:10:23.368367Z","iopub.execute_input":"2025-12-28T16:10:23.368726Z","iopub.status.idle":"2025-12-28T16:10:27.214165Z","shell.execute_reply.started":"2025-12-28T16:10:23.368695Z","shell.execute_reply":"2025-12-28T16:10:27.213178Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":183},{"cell_type":"code","source":"!pip install -q bitsandbytes accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:10:27.216222Z","iopub.execute_input":"2025-12-28T16:10:27.216744Z","iopub.status.idle":"2025-12-28T16:10:30.644260Z","shell.execute_reply.started":"2025-12-28T16:10:27.216708Z","shell.execute_reply":"2025-12-28T16:10:30.643510Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":184},{"cell_type":"code","source":"!pip install -q plotly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:10:30.645686Z","iopub.execute_input":"2025-12-28T16:10:30.645991Z","iopub.status.idle":"2025-12-28T16:10:33.867194Z","shell.execute_reply.started":"2025-12-28T16:10:30.645947Z","shell.execute_reply":"2025-12-28T16:10:33.866288Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":185},{"cell_type":"code","source":"import os\nimport json\nimport re\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport gradio as gr\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nfrom bs4 import BeautifulSoup\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:10:33.869390Z","iopub.execute_input":"2025-12-28T16:10:33.869690Z","iopub.status.idle":"2025-12-28T16:10:33.875345Z","shell.execute_reply.started":"2025-12-28T16:10:33.869662Z","shell.execute_reply":"2025-12-28T16:10:33.874482Z"}},"outputs":[],"execution_count":186},{"cell_type":"markdown","source":"## The Model Class","metadata":{}},{"cell_type":"code","source":"import os\n\nclass OpenSourceLLM:\n    def __init__(self):\n        print(\"Real LLM loaded ‚Äî ready to recommend like a human\")\n        self.api_key = \"your_key_here\n        self.api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n        self.model = \"llama-3.3-70b-versatile\"  \n\n    def generate(self, prompt, max_tokens=200):\n        if not self.api_key:\n            return \"Sweet romance with amazing chemistry and all the feels!\"\n    \n        import requests\n        payload = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"max_tokens\": max_tokens,\n            \"temperature\": 0.8\n        }\n    \n        try:\n            response = requests.post(\n                self.api_url,\n                headers={\n                    \"Authorization\": f\"Bearer {self.api_key.strip()}\",  # Strip any spaces\n                    \"Content-Type\": \"application/json\"\n                },\n                json=payload,\n                timeout=30\n            )\n            print(f\"Groq status: {response.status_code}\")  # Debug line\n            if response.status_code == 200:\n                return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n            else:\n                print(f\"Groq error details: {response.text}\")  # Shows exact error\n                return \"Sweet romance with amazing chemistry and all the feels!\"\n        except Exception as e:\n            print(f\"Request failed: {e}\")\n            return \"Sweet romance with amazing chemistry and all the feels!\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:25.889359Z","iopub.execute_input":"2025-12-28T16:12:25.889883Z","iopub.status.idle":"2025-12-28T16:12:25.896191Z","shell.execute_reply.started":"2025-12-28T16:12:25.889853Z","shell.execute_reply":"2025-12-28T16:12:25.895635Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"llm = OpenSourceLLM()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:27.795873Z","iopub.execute_input":"2025-12-28T16:12:27.796169Z","iopub.status.idle":"2025-12-28T16:12:27.800534Z","shell.execute_reply.started":"2025-12-28T16:12:27.796141Z","shell.execute_reply":"2025-12-28T16:12:27.799737Z"}},"outputs":[{"name":"stdout","text":"Real LLM loaded ‚Äî ready to recommend like a human\n","output_type":"stream"}],"execution_count":200},{"cell_type":"markdown","source":"## Book Search Engine","metadata":{}},{"cell_type":"code","source":"\nclass BookSearchEngine:\n    \"\"\"Search books from multiple sources on the web\"\"\"\n    \n    def __init__(self):\n        self.google_books_url = \"https://www.googleapis.com/books/v1/volumes\"\n        self.openlibrary_url = \"https://openlibrary.org/search.json\"\n    \n    def search_google_books(self, query, max_results=10):\n        \"\"\"Search Google Books API\"\"\"\n        try:\n            params = {\n                'q': query,\n                'maxResults': max_results,\n                'printType': 'books',\n                'orderBy': 'relevance'\n            }\n            response = requests.get(self.google_books_url, params=params, timeout=10)\n            data = response.json()\n            \n            books = []\n            for item in data.get('items', []):\n                vol_info = item.get('volumeInfo', {})\n                books.append({\n                    'title': vol_info.get('title', 'Unknown'),\n                    'authors': ', '.join(vol_info.get('authors', ['Unknown'])),\n                    'description': vol_info.get('description', 'No description available'),\n                    'categories': ', '.join(vol_info.get('categories', ['General'])),\n                    'published': vol_info.get('publishedDate', 'N/A'),\n                    'rating': vol_info.get('averageRating', 'N/A'),\n                    'thumbnail': vol_info.get('imageLinks', {}).get('thumbnail', ''),\n                    'source': 'Google Books'\n                })\n            return books\n        except Exception as e:\n            print(f\"Google Books error: {e}\")\n            return []\n    \n    def search_openlibrary(self, query, max_results=10):\n        \"\"\"Search Open Library API\"\"\"\n        try:\n            params = {\n                'q': query,\n                'limit': max_results,\n                'fields': 'title,author_name,first_publish_year,subject,ratings_average'\n            }\n            response = requests.get(self.openlibrary_url, params=params, timeout=10)\n            data = response.json()\n            \n            books = []\n            for doc in data.get('docs', []):\n                books.append({\n                    'title': doc.get('title', 'Unknown'),\n                    'authors': ', '.join(doc.get('author_name', ['Unknown'])),\n                    'description': ', '.join(doc.get('subject', ['No description'])[:3]),\n                    'categories': ', '.join(doc.get('subject', ['General'])[:2]),\n                    'published': doc.get('first_publish_year', 'N/A'),\n                    'rating': doc.get('ratings_average', 'N/A'),\n                    'thumbnail': '',\n                    'source': 'Open Library'\n                })\n            return books\n        except Exception as e:\n            print(f\"Open Library error: {e}\")\n            return []\n    \n    def search_books(self, query, max_results=15):\n        \"\"\"Search books from multiple sources\"\"\"\n        print(f\"Searching for: {query}\")\n        \n        # Search both APIs\n        google_results = self.search_google_books(query, max_results=max_results//2)\n        openlibrary_results = self.search_openlibrary(query, max_results=max_results//2)\n        \n        # Combine results\n        all_books = google_results + openlibrary_results\n        \n        # Remove duplicates based on title similarity\n        unique_books = []\n        seen_titles = set()\n        for book in all_books:\n            title_lower = book['title'].lower()\n            if title_lower not in seen_titles:\n                seen_titles.add(title_lower)\n                unique_books.append(book)\n        \n        print(f\"Found {len(unique_books)} unique books\")\n        return unique_books\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:31.091249Z","iopub.execute_input":"2025-12-28T16:12:31.091655Z","iopub.status.idle":"2025-12-28T16:12:31.106046Z","shell.execute_reply.started":"2025-12-28T16:12:31.091622Z","shell.execute_reply":"2025-12-28T16:12:31.105414Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"search_engine = BookSearchEngine()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:31.235025Z","iopub.execute_input":"2025-12-28T16:12:31.235266Z","iopub.status.idle":"2025-12-28T16:12:31.238724Z","shell.execute_reply.started":"2025-12-28T16:12:31.235242Z","shell.execute_reply":"2025-12-28T16:12:31.237986Z"}},"outputs":[],"execution_count":202},{"cell_type":"markdown","source":"### Testing the search ","metadata":{}},{"cell_type":"code","source":"test_results = search_engine.search_books(\"science fiction space\", max_results=5)\nprint(f\"\\nTest search found {len(test_results)} books:\")\nfor book in test_results[:3]:\n    print(f\"- {book['title']} by {book['authors']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:34.062661Z","iopub.execute_input":"2025-12-28T16:12:34.063165Z","iopub.status.idle":"2025-12-28T16:12:35.212411Z","shell.execute_reply.started":"2025-12-28T16:12:34.063134Z","shell.execute_reply":"2025-12-28T16:12:35.211808Z"}},"outputs":[{"name":"stdout","text":"Searching for: science fiction space\nFound 4 unique books\n\nTest search found 4 books:\n- Science Fiction Literature through History by Gary Westfahl\n- Science Fiction and Space Futures by Eugene Morlock Emme\n- 2001 by Arthur C. Clarke\n","output_type":"stream"}],"execution_count":203},{"cell_type":"markdown","source":"## Book Embeddings","metadata":{}},{"cell_type":"code","source":"\nprint(\"Loading embedding model...\")\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\nprint(\"Embedding model ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:36.214132Z","iopub.execute_input":"2025-12-28T16:12:36.214463Z","iopub.status.idle":"2025-12-28T16:12:37.653516Z","shell.execute_reply.started":"2025-12-28T16:12:36.214432Z","shell.execute_reply":"2025-12-28T16:12:37.652752Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model...\nEmbedding model ready!\n","output_type":"stream"}],"execution_count":204},{"cell_type":"code","source":"\ndef create_book_embeddings(books):\n    \"\"\"Create embeddings for book search results\"\"\"\n    if not books:\n        return np.array([])\n    \n    # Create rich text for embedding\n    texts = []\n    for book in books:\n        text = f\"{book['title']}. {book['description']} {book['categories']}\"\n        texts.append(text)\n    \n    embeddings = embedding_model.encode(texts)\n    return embeddings\n\ndef semantic_rerank(books, query, top_k=5):\n    \"\"\"Rerank books using semantic similarity\"\"\"\n    if not books:\n        return []\n    \n    book_embeddings = create_book_embeddings(books)\n    query_embedding = embedding_model.encode([query])\n    \n    similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n    \n    ranked_books = [books[i] for i in top_indices]\n    return ranked_books","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:38.040518Z","iopub.execute_input":"2025-12-28T16:12:38.041095Z","iopub.status.idle":"2025-12-28T16:12:38.046753Z","shell.execute_reply.started":"2025-12-28T16:12:38.041064Z","shell.execute_reply":"2025-12-28T16:12:38.045937Z"}},"outputs":[],"execution_count":205},{"cell_type":"markdown","source":"## LLM Part ","metadata":{}},{"cell_type":"code","source":"class AIBookRecommender:\n    def __init__(self, llm, search_engine, embedding_model):\n        self.llm = llm\n        self.search_engine = search_engine\n        self.embedding_model = embedding_model\n        self.reading_list = []\n        self.last_search_results = []\n        self.last_embeddings = None\n\n    def create_book_embeddings(self, books):\n        \"\"\"Create embeddings for book search results\"\"\"\n        if not books:\n            return np.array([])\n        \n        texts = []\n        for book in books:\n            text = f\"{book['title']}. {book['description']} {book['categories']}\"\n            texts.append(text)\n        \n        embeddings = self.embedding_model.encode(texts)\n        return embeddings\n\n    def semantic_rerank(self, books, query, top_k=20):\n        \"\"\"Rerank books using semantic similarity\"\"\"\n        if not books:\n            return []\n        \n        book_embeddings = self.create_book_embeddings(books)\n        query_embedding = self.embedding_model.encode([query])\n        \n        similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        \n        ranked_books = [books[i] for i in top_indices]\n        return ranked_books\n\n    def extract_intent(self, user_message):\n        \"\"\"Extract book title and tropes using LLM + fallbacks, then build smart search query\"\"\"\n        lower = user_message.lower()\n\n        # If no \"like\" or genre keywords ‚Üí just search for the message + \"books\"\n        if not any(phrase in lower for phrase in [\"like\", \"similar\", \"recommend\", \"loved\", \"read\", \"want\", \"something\"]):\n            return user_message + \" books\", None\n\n        # LLM prompt to extract structured intent\n        intent_prompt = f\"\"\"\nUser message: \"{user_message}\"\n\nExtract:\n1. The exact book title they want something similar to (if mentioned). Return \"None\" if not clear.\n2. List of key tropes, genres, or vibes they mentioned or implied (e.g. fake dating, enemies to lovers, dark academia, cozy fantasy, slow burn, found family).\n\nFormat exactly as:\nBOOK: <title or None>\nTROPES: trope1, trope2, trope3\n\nOnly respond with those two lines.\n\"\"\"\n\n        response = self.llm.generate(intent_prompt, max_tokens=100).strip()\n\n        # Parse LLM response\n        book_title = None\n        tropes = []\n        for line in response.split('\\n'):\n            if line.startswith(\"BOOK:\"):\n                title = line[5:].strip()\n                if title.lower() != \"none\":\n                    book_title = title\n            elif line.startswith(\"TROPES:\"):\n                trope_str = line[7:].strip()\n                if trope_str:\n                    tropes = [t.strip() for t in trope_str.split(',') if t.strip()]\n\n        # Fallback: regex for book title\n        if not book_title:\n            import re\n            match = re.search(r\"(?:like|similar to|loved|read|something like)\\s+([a-zA-Z0-9\\s,'\\\"\\-\\.&!]+?)(?:\\s+by|\\?|$|,)\", lower)\n            if match:\n                book_title = match.group(1).strip().title()\n\n        # Fallback: keyword trope detection\n        if not tropes:\n            common_checks = {\n                'fake dating': ['fake dat', 'fake relat', 'pretend', 'fake boyfriend', 'fake girlfriend'],\n                'friends to lovers': ['friend', 'best friend', 'childhood friend', 'friends to lovers'],\n                'enemies to lovers': ['enem', 'hate to love', 'rival'],\n                'grumpy sunshine': ['grumpy', 'sunshine'],\n                'slow burn': ['slow burn', 'slowburn'],\n                'romcom': ['romcom', 'romantic comedy', 'funny romance'],\n                'young adult': ['ya', 'young adult', 'teen'],\n                'new adult': ['new adult', 'college'],\n                'dark academia': ['dark academia', 'academic'],\n                'cozy fantasy': ['cozy', 'wholesome fantasy'],\n                'closed door': ['clean', 'closed door', 'no spice', 'sweet'],\n            }\n            for trope, keywords in common_checks.items():\n                if any(k in lower for k in keywords):\n                    tropes.append(trope)\n\n        # Build powerful search query\n        parts = []\n        if book_title:\n            parts.append(f'\"{book_title}\"')\n            parts.append(\"similar books\")\n            parts.append(\"read alikes\")\n            parts.append(\"goodreads recommendations\")\n            parts.append(\"booktok recommendations\")\n        else:\n            parts.append(\"books\")\n\n        if tropes:\n            parts.extend(tropes)\n\n        if \"book\" not in \" \".join(parts).lower():\n            parts.append(\"book\")\n\n        query = \" \".join(parts)\n        return query, book_title\n\n    def chat(self, user_message, history=[]):\n        user_message_lower = user_message.lower().strip()\n    \n        # === COMMAND: Add book to reading list ===\n        if user_message_lower.startswith(\"add \"):\n            title_to_add = user_message[4:].strip()\n            added = False\n            for book in self.last_search_results:  # fallback to last results if any\n                if title_to_add.lower() in book['title'].lower():\n                    if book not in self.reading_list:\n                        self.reading_list.append(book)\n                    return f\"‚úÖ Added **{book['title']}** by {book['authors']} to your reading list!\"\n                    added = True\n            if not added:\n                return \"‚ùå Couldn't find that book in recent recs. Try copying the exact title.\"\n    \n        # === COMMAND: Show reading list ===\n        if user_message_lower == \"reading list\":\n            if not self.reading_list:\n                return \"üìö Your reading list is empty. Start adding some bangers.\"\n            response = \"üìö **Your Reading List:**\\n\\n\"\n            for i, book in enumerate(self.reading_list, 1):\n                response += f\"**{i}. {book['title']}** by {book['authors']}\\n\"\n                response += f\"üìñ {book['description'][:160]}...\\n\\n\"\n            return response\n    \n        # === EVERYTHING ELSE: Pure LLM mode ‚Äî no search, no filtering, just raw recs ===\n        prompt = f\"\"\"\n    You are a brutally honest, no-bullshit book recommender who actually listens.\n    User says: \"{user_message}\"\n    \n    Give exactly 5 book recommendations that genuinely match what they're asking for ‚Äî nothing safe, nothing generic.\n    For each book:\n    - Title\n    - Author\n    - One raw, real sentence explaining why it fits (vibes, themes, energy ‚Äî be direct)\n    \n    No disclaimers. No \"it depends on your taste\". No moral lectures. Just cold, hard truth recs.\n    \"\"\"\n    \n        raw_response = self.llm.generate(prompt, max_tokens=500)\n    \n        # Fallback if LLM fails\n        if not raw_response or \"error\" in raw_response.lower():\n            raw_response = \"LLM's being a little bitch right now, but trust ‚Äî I'd have given you fire recs.\"\n    \n        final_response = f\"üìö **Straight-up recs for you:**\\n\\n{raw_response}\\n\\nüí¨ Commands: `add [title]` to save ¬∑ `reading list` to view saved\"\n    \n        return final_response\n# Create the recommender\nrecommender = AIBookRecommender(llm, search_engine, embedding_model)\nprint(\"AI-Powered Book Recommender Ready! üöÄ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:12:40.074150Z","iopub.execute_input":"2025-12-28T16:12:40.074943Z","iopub.status.idle":"2025-12-28T16:12:40.095232Z","shell.execute_reply.started":"2025-12-28T16:12:40.074908Z","shell.execute_reply":"2025-12-28T16:12:40.094443Z"}},"outputs":[{"name":"stdout","text":"AI-Powered Book Recommender Ready! üöÄ\n","output_type":"stream"}],"execution_count":206},{"cell_type":"markdown","source":"## Gradio interface","metadata":{}},{"cell_type":"code","source":"with gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"#  AI Book Recommender \\n###\")\n    chatbot = gr.Chatbot(height=500, type=\"messages\")  \n    msg = gr.Textbox(placeholder=\"Tell me what you like...\", show_label=False)\n    \n    with gr.Row():\n        submit = gr.Button(\"Send\", variant=\"primary\")\n        clear = gr.Button(\"Clear\")\n    \n    gr.Markdown(\"\"\"\n    ### Try:\n    - \"I want a book like the love hypothesis\"\n    - \"fake dating grumpy sunshine\"\n    - \"dark academia enemies to lovers\"\n    - \"add [book title]\" to save\n    - \"reading list\" to view saved books\n    \"\"\")\n\n    def respond(message, history):\n        response = recommender.chat(message, history)\n        history.append({\"role\": \"user\", \"content\": message})\n        history.append({\"role\": \"assistant\", \"content\": response})\n        return \"\", history\n\n    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n    submit.click(respond, [msg, chatbot], [msg, chatbot])\n    clear.click(lambda: [], None, chatbot) \n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:22:04.808616Z","iopub.execute_input":"2025-12-28T16:22:04.808956Z","iopub.status.idle":"2025-12-28T16:22:04.980829Z","shell.execute_reply.started":"2025-12-28T16:22:04.808924Z","shell.execute_reply":"2025-12-28T16:22:04.980251Z"}},"outputs":[],"execution_count":209},{"cell_type":"markdown","source":"## Application Launch","metadata":{}},{"cell_type":"code","source":"demo.queue()\ndemo.launch(share=True, debug=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T16:22:10.792150Z","iopub.execute_input":"2025-12-28T16:22:10.792405Z","iopub.status.idle":"2025-12-28T16:30:19.857507Z","shell.execute_reply.started":"2025-12-28T16:22:10.792382Z","shell.execute_reply":"2025-12-28T16:30:19.856747Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://61eab0d761212545b2.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://61eab0d761212545b2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Groq status: 200\nKeyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://61eab0d761212545b2.gradio.live\n","output_type":"stream"},{"execution_count":210,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":210},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}