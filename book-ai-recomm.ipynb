{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:17.307276Z",
     "iopub.status.busy": "2025-12-29T18:08:17.306538Z",
     "iopub.status.idle": "2025-12-29T18:08:21.067769Z",
     "shell.execute_reply": "2025-12-29T18:08:21.066945Z",
     "shell.execute_reply.started": "2025-12-29T18:08:17.307241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch sentence-transformers gradio requests beautifulsoup4 langchain langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:21.069625Z",
     "iopub.status.busy": "2025-12-29T18:08:21.069307Z",
     "iopub.status.idle": "2025-12-29T18:08:24.357363Z",
     "shell.execute_reply": "2025-12-29T18:08:24.356584Z",
     "shell.execute_reply.started": "2025-12-29T18:08:21.069592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:24.358771Z",
     "iopub.status.busy": "2025-12-29T18:08:24.358439Z",
     "iopub.status.idle": "2025-12-29T18:08:27.505341Z",
     "shell.execute_reply": "2025-12-29T18:08:27.504562Z",
     "shell.execute_reply.started": "2025-12-29T18:08:24.358730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:08:27.507635Z",
     "iopub.status.busy": "2025-12-29T18:08:27.507360Z",
     "iopub.status.idle": "2025-12-29T18:08:27.513210Z",
     "shell.execute_reply": "2025-12-29T18:08:27.512222Z",
     "shell.execute_reply.started": "2025-12-29T18:08:27.507607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:18.008330Z",
     "iopub.status.busy": "2025-12-29T18:10:18.008030Z",
     "iopub.status.idle": "2025-12-29T18:10:18.015284Z",
     "shell.execute_reply": "2025-12-29T18:10:18.014566Z",
     "shell.execute_reply.started": "2025-12-29T18:10:18.008300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class OpenSourceLLM:\n",
    "    def __init__(self):\n",
    "        print(\"Real LLM loaded â€” ready to recommend like a human\")\n",
    "        self.api_key = \"your_key_here\n",
    "        self.api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        self.model = \"llama-3.3-70b-versatile\"  \n",
    "\n",
    "    def generate(self, prompt, max_tokens=200):\n",
    "        if not self.api_key:\n",
    "            return \"Sweet romance with amazing chemistry and all the feels!\"\n",
    "    \n",
    "        import requests\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {self.api_key.strip()}\",  # Strip any spaces\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            print(f\"Groq status: {response.status_code}\")  # Debug line\n",
    "            if response.status_code == 200:\n",
    "                return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                print(f\"Groq error details: {response.text}\")  # Shows exact error\n",
    "                return \"Sweet romance with amazing chemistry and all the feels!\"\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return \"Sweet romance with amazing chemistry and all the feels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:20.155830Z",
     "iopub.status.busy": "2025-12-29T18:10:20.155203Z",
     "iopub.status.idle": "2025-12-29T18:10:20.159808Z",
     "shell.execute_reply": "2025-12-29T18:10:20.158988Z",
     "shell.execute_reply.started": "2025-12-29T18:10:20.155797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real LLM loaded â€” ready to recommend like a human\n"
     ]
    }
   ],
   "source": [
    "llm = OpenSourceLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:20.631570Z",
     "iopub.status.busy": "2025-12-29T18:10:20.630893Z",
     "iopub.status.idle": "2025-12-29T18:10:20.643340Z",
     "shell.execute_reply": "2025-12-29T18:10:20.642519Z",
     "shell.execute_reply.started": "2025-12-29T18:10:20.631538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BookSearchEngine:\n",
    "    \"\"\"Search books from multiple sources on the web\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.google_books_url = \"https://www.googleapis.com/books/v1/volumes\"\n",
    "        self.openlibrary_url = \"https://openlibrary.org/search.json\"\n",
    "    \n",
    "    def search_google_books(self, query, max_results=10):\n",
    "        \"\"\"Search Google Books API\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'maxResults': max_results,\n",
    "                'printType': 'books',\n",
    "                'orderBy': 'relevance'\n",
    "            }\n",
    "            response = requests.get(self.google_books_url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            books = []\n",
    "            for item in data.get('items', []):\n",
    "                vol_info = item.get('volumeInfo', {})\n",
    "                books.append({\n",
    "                    'title': vol_info.get('title', 'Unknown'),\n",
    "                    'authors': ', '.join(vol_info.get('authors', ['Unknown'])),\n",
    "                    'description': vol_info.get('description', 'No description available'),\n",
    "                    'categories': ', '.join(vol_info.get('categories', ['General'])),\n",
    "                    'published': vol_info.get('publishedDate', 'N/A'),\n",
    "                    'rating': vol_info.get('averageRating', 'N/A'),\n",
    "                    'thumbnail': vol_info.get('imageLinks', {}).get('thumbnail', ''),\n",
    "                    'source': 'Google Books'\n",
    "                })\n",
    "            return books\n",
    "        except Exception as e:\n",
    "            print(f\"Google Books error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_openlibrary(self, query, max_results=10):\n",
    "        \"\"\"Search Open Library API\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'limit': max_results,\n",
    "                'fields': 'title,author_name,first_publish_year,subject,ratings_average'\n",
    "            }\n",
    "            response = requests.get(self.openlibrary_url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            books = []\n",
    "            for doc in data.get('docs', []):\n",
    "                books.append({\n",
    "                    'title': doc.get('title', 'Unknown'),\n",
    "                    'authors': ', '.join(doc.get('author_name', ['Unknown'])),\n",
    "                    'description': ', '.join(doc.get('subject', ['No description'])[:3]),\n",
    "                    'categories': ', '.join(doc.get('subject', ['General'])[:2]),\n",
    "                    'published': doc.get('first_publish_year', 'N/A'),\n",
    "                    'rating': doc.get('ratings_average', 'N/A'),\n",
    "                    'thumbnail': '',\n",
    "                    'source': 'Open Library'\n",
    "                })\n",
    "            return books\n",
    "        except Exception as e:\n",
    "            print(f\"Open Library error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_books(self, query, max_results=15):\n",
    "        \"\"\"Search books from multiple sources\"\"\"\n",
    "        print(f\"Searching for: {query}\")\n",
    "        \n",
    "        # Search both APIs\n",
    "        google_results = self.search_google_books(query, max_results=max_results//2)\n",
    "        openlibrary_results = self.search_openlibrary(query, max_results=max_results//2)\n",
    "        \n",
    "        # Combine results\n",
    "        all_books = google_results + openlibrary_results\n",
    "        \n",
    "        # Remove duplicates based on title similarity\n",
    "        unique_books = []\n",
    "        seen_titles = set()\n",
    "        for book in all_books:\n",
    "            title_lower = book['title'].lower()\n",
    "            if title_lower not in seen_titles:\n",
    "                seen_titles.add(title_lower)\n",
    "                unique_books.append(book)\n",
    "        \n",
    "        print(f\"Found {len(unique_books)} unique books\")\n",
    "        return unique_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:20.827952Z",
     "iopub.status.busy": "2025-12-29T18:10:20.827740Z",
     "iopub.status.idle": "2025-12-29T18:10:20.831115Z",
     "shell.execute_reply": "2025-12-29T18:10:20.830550Z",
     "shell.execute_reply.started": "2025-12-29T18:10:20.827931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "search_engine = BookSearchEngine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:21.276534Z",
     "iopub.status.busy": "2025-12-29T18:10:21.276186Z",
     "iopub.status.idle": "2025-12-29T18:10:22.497057Z",
     "shell.execute_reply": "2025-12-29T18:10:22.496371Z",
     "shell.execute_reply.started": "2025-12-29T18:10:21.276504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: science fiction space\n",
      "Found 4 unique books\n",
      "\n",
      "Test search found 4 books:\n",
      "- Deep Space; Eight Stories of Science Fiction by Robert Silverberg\n",
      "- Science Fiction and Space Futures by Eugene Morlock Emme\n",
      "- 2001 by Arthur C. Clarke\n"
     ]
    }
   ],
   "source": [
    "test_results = search_engine.search_books(\"science fiction space\", max_results=5)\n",
    "print(f\"\\nTest search found {len(test_results)} books:\")\n",
    "for book in test_results[:3]:\n",
    "    print(f\"- {book['title']} by {book['authors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:22.498518Z",
     "iopub.status.busy": "2025-12-29T18:10:22.498245Z",
     "iopub.status.idle": "2025-12-29T18:10:25.339940Z",
     "shell.execute_reply": "2025-12-29T18:10:25.339139Z",
     "shell.execute_reply.started": "2025-12-29T18:10:22.498464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e98808aaa0643928dcc41ea7ba0a090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892cb8e930cf4ff6858a5c5846ed69b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af540bd8d965454ba40602935aedbb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bba11fb76948b49604be66a40a5f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4770870e62b44567abb5654f03b1bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf8c313c8ce45aaae1de4a443703727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cfa156189845048636fde5aebf76d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f4f228f68c4dada2031580055b5053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded462f0c87e4765afbe3306e0af3b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444b4f22262849a487c7ef65e5e55618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5eb8c49d424c0682e25fab48ddb61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:25.341114Z",
     "iopub.status.busy": "2025-12-29T18:10:25.340883Z",
     "iopub.status.idle": "2025-12-29T18:10:25.348475Z",
     "shell.execute_reply": "2025-12-29T18:10:25.347732Z",
     "shell.execute_reply.started": "2025-12-29T18:10:25.341091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_book_embeddings(books):\n",
    "    \"\"\"Create embeddings for book search results\"\"\"\n",
    "    if not books:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Create rich text for embedding\n",
    "    texts = []\n",
    "    for book in books:\n",
    "        text = f\"{book['title']}. {book['description']} {book['categories']}\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def semantic_rerank(books, query, top_k=5):\n",
    "    \"\"\"Rerank books using semantic similarity\"\"\"\n",
    "    if not books:\n",
    "        return []\n",
    "    \n",
    "    book_embeddings = create_book_embeddings(books)\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    \n",
    "    similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    ranked_books = [books[i] for i in top_indices]\n",
    "    return ranked_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:30.719502Z",
     "iopub.status.busy": "2025-12-29T18:10:30.718885Z",
     "iopub.status.idle": "2025-12-29T18:10:30.738115Z",
     "shell.execute_reply": "2025-12-29T18:10:30.735939Z",
     "shell.execute_reply.started": "2025-12-29T18:10:30.719450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-Powered Book Recommender Ready! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "class AIBookRecommender:\n",
    "    def __init__(self, llm, search_engine, embedding_model):\n",
    "        self.llm = llm\n",
    "        self.search_engine = search_engine\n",
    "        self.embedding_model = embedding_model\n",
    "        self.reading_list = []\n",
    "        self.last_search_results = []\n",
    "        self.last_embeddings = None\n",
    "\n",
    "    def create_book_embeddings(self, books):\n",
    "        \"\"\"Create embeddings for book search results\"\"\"\n",
    "        if not books:\n",
    "            return np.array([])\n",
    "        \n",
    "        texts = []\n",
    "        for book in books:\n",
    "            text = f\"{book['title']}. {book['description']} {book['categories']}\"\n",
    "            texts.append(text)\n",
    "        \n",
    "        embeddings = self.embedding_model.encode(texts)\n",
    "        return embeddings\n",
    "\n",
    "    def semantic_rerank(self, books, query, top_k=20):\n",
    "        \"\"\"Rerank books using semantic similarity\"\"\"\n",
    "        if not books:\n",
    "            return []\n",
    "        \n",
    "        book_embeddings = self.create_book_embeddings(books)\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, book_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        ranked_books = [books[i] for i in top_indices]\n",
    "        return ranked_books\n",
    "\n",
    "    \n",
    "    def chat(self, user_message, history=[]):\n",
    "        user_message_lower = user_message.lower().strip()\n",
    "    \n",
    "        # Commands\n",
    "        if user_message_lower.startswith(\"add \"):\n",
    "            # your existing add logic\n",
    "            pass\n",
    "        elif user_message_lower in [\"reading list\", \"my list\"]:\n",
    "            # your existing reading list logic\n",
    "            pass\n",
    "        else:\n",
    "            # FULL LLM MODE â€” this is the magic\n",
    "            prompt = f\"\"\"\n",
    "    You are the world's best book recommender in 2025. You know every BookTok trend, Goodreads list, and reader preference.\n",
    "    \n",
    "    User wants: \"{user_message}\"\n",
    "    \n",
    "    Recommend exactly 5 books that REAL readers say are the closest matches.\n",
    "    Rules:\n",
    "    - If a specific book is mentioned â†’ prioritize same author + exact trope matches\n",
    "    - Always respect age group: YA for teens, adult for mature\n",
    "    - Use current trends: BookTok hits, popular 2024-2025 releases\n",
    "    - No children's books, textbooks, or random classics unless asked\n",
    "    \n",
    "    Format:\n",
    "    1. \"Title\" by Author\n",
    "       â†’ One sharp sentence why it's perfect\n",
    "    \n",
    "    Examples:\n",
    "    \"betting on you\" â†’ Lynn Painter books first (Better Than the Movies, The Do-Over, etc.)\n",
    "    \"the housemaid\" â†’ Freida McFadden thrillers (Never Lie, The Teacher, etc.)\n",
    "    \"love hypothesis\" â†’ Ali Hazelwood + Spanish Love Deception\n",
    "    \"\"\"\n",
    "    \n",
    "            recommendations = self.llm.generate(prompt, max_tokens=500).strip()\n",
    "    \n",
    "            # Optional: Fetch covers for visual polish (doesn't affect quality)\n",
    "            try:\n",
    "                cover_books = self.search_engine.search_books(user_message + \" book\", max_results=20)\n",
    "                ranked = self.semantic_rerank(cover_books, user_message, top_k=5)\n",
    "                self.last_search_results = ranked\n",
    "            except:\n",
    "                self.last_search_results = []\n",
    "    \n",
    "            response = f\"ðŸ“š **Perfect matches for: \\\"{user_message}\\\"**\\n\\n\"\n",
    "            response += recommendations + \"\\n\\n\"\n",
    "            if self.last_search_results:\n",
    "                response += \"ðŸ“– **With covers:**\\n\\n\"\n",
    "                for book in self.last_search_results[:3]:\n",
    "                    response += f\"**{book['title']}** by {book['authors']}\\n\"\n",
    "                    if book['thumbnail']:\n",
    "                        response += f\"![cover]({book['thumbnail']})\\n\"\n",
    "                    response += \"\\n\"\n",
    "    \n",
    "            response += \"\\nðŸ’¬ Use `add [title]` to save â€¢ `reading list` to view\"\n",
    "            return response\n",
    "# Create the recommender\n",
    "recommender = AIBookRecommender(llm, search_engine, embedding_model)\n",
    "print(\"AI-Powered Book Recommender Ready! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:46.323027Z",
     "iopub.status.busy": "2025-12-29T18:10:46.322246Z",
     "iopub.status.idle": "2025-12-29T18:10:46.507935Z",
     "shell.execute_reply": "2025-12-29T18:10:46.507149Z",
     "shell.execute_reply.started": "2025-12-29T18:10:46.322993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"#  AI Book Recommender \\n###\")\n",
    "    chatbot = gr.Chatbot(height=500, type=\"messages\")  \n",
    "    msg = gr.Textbox(placeholder=\"Tell me what you like...\", show_label=False)\n",
    "    \n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Send\", variant=\"primary\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ### Try:\n",
    "    - \"I want a book like the love hypothesis\"\n",
    "    - \"fake dating grumpy sunshine\"\n",
    "    - \"dark academia enemies to lovers\"\n",
    "    - \"add [book title]\" to save\n",
    "    - \"reading list\" to view saved books\n",
    "    \"\"\")\n",
    "\n",
    "    def respond(message, history):\n",
    "        if not history: \n",
    "            history.append({\"role\": \"assistant\", \"content\": \"Hey! I'm your AI book recommender. Tell me a book you loved or a vibe you're craving! ðŸ“š\"})\n",
    "        response = recommender.chat(message, history)\n",
    "        history.append({\"role\": \"user\", \"content\": message})\n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return \"\", history\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: [], None, chatbot) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-29T18:10:48.017374Z",
     "iopub.status.busy": "2025-12-29T18:10:48.017064Z",
     "iopub.status.idle": "2025-12-29T18:16:13.062032Z",
     "shell.execute_reply": "2025-12-29T18:16:13.061272Z",
     "shell.execute_reply.started": "2025-12-29T18:10:48.017343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://829f9f0313bf9c83d1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://829f9f0313bf9c83d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq status: 200\n",
      "Searching for: hey book\n",
      "Found 20 unique books\n",
      "Groq status: 200\n",
      "Searching for: i loved the housemaid what can u suggested? book\n",
      "Found 10 unique books\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://829f9f0313bf9c83d1.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.queue()\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
